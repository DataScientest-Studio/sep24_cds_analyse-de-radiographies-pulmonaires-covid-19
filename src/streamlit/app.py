import streamlit as st
import tensorflow as tf
import numpy as np
from PIL import Image
import pandas as pd  
import plotly.express as px
import plotly.graph_objects as go
import os


st.write("R√©pertoire courant :", os.getcwd())
st.write("Contenu du r√©pertoire :", os.listdir())
st.write("Contenu de ../images :", os.listdir("../images") if os.path.exists("../images") else "Dossier 'images' non trouv√©")

# Configuration de la page
st.set_page_config(page_title="Analyse COVID-19 Radiographies", layout="wide")

col1, col2 = st.columns([1, 10])  # Puedes ajustar la proporci√≥n
with col1:
    st.image("../images/Normal-2.png", width=170)
with col2:
    st.title("D√©tection de pathologies pulmonaires √† partir de radiographies thoraciques")

st.markdown("**Projet bas√© sur le rapport d'analyse de radiographies pour la d√©tection de la COVID-19 et autres pathologies.**")

# Menu de navigation
section = st.sidebar.radio("\U0001F4DA Navigation", [
    "1. Introduction",
    "2. Analyse des donn√©es",
    "3. M√©thodologie",
    "4. Modelisation",
    "5. Mod√®les de Deep Learning",
    "6. Mod√®les Avanc√©s",
    "7. R√©sultats et Interpr√©tation",
    "8. Bilan et perspectives",
    "9. Essai avec une radiographie"
])

def interactive_image(path, caption=None):
    img = Image.open(path)
    width, height = img.size
    fig = go.Figure()

    fig.add_layout_image(
        dict(
            source=img,
            x=0, y=height,
            sizex=width, sizey=height,
            xref="x", yref="y",
            sizing="contain",
            layer="below"
        )
    )

    fig.update_xaxes(visible=False, range=[0, width])
    fig.update_yaxes(visible=False, range=[0, height])
    fig.update_layout(
        width=width,
        height=height,
        margin=dict(l=0, r=0, t=0, b=0),
        dragmode="zoom",
        title=caption if caption else ""
    )

    st.plotly_chart(fig, use_container_width=True)

# Sections
if section == "1. Introduction":
    st.title("Introduction")
    st.write("""
    Le diagnostic m√©dical par imagerie radiologique est un √©l√©ment cl√© de la m√©decine moderne. Toutefois, l‚Äôinterpr√©tation manuelle des radiographies thoraciques est souvent complexe, sujette √† erreurs humaines et d√©pend fortement de l‚Äôexpertise du radiologue.

    Ce projet, r√©alis√© dans le cadre de la formation Data Scientist - Septembre 2024 chez Data Scientest, vise √† d√©velopper un pipeline automatique de classification des radiographies pulmonaires. L‚Äôobjectif principal est la d√©tection pr√©cise de la COVID-19, de la pneumonie virale et d'autres pathologies pulmonaires.

    La m√©thodologie comprend une exploration et un pr√©-traitement approfondis des donn√©es, suivi de la comparaison entre mod√®les classiques de Machine Learning et architectures avanc√©es de Deep Learning, incluant des techniques de transfer learning et des mod√®les hybrides CNN-ViT.

    Les performances des mod√®les sont √©valu√©es via plusieurs m√©triques cl√©s : pr√©cision, sensibilit√© (rappel), sp√©cificit√©, F1-score pond√©r√© et matrice de confusion.
    """)


elif section == "2. Analyse des donn√©es":
    st.title("Analyse des donn√©es")
    st.subheader("Description du jeu de donn√©es")
    st.write("""
    Le jeu de donn√©es comprend 21 164 images r√©parties entre quatre classes : Normal (10192), COVID-19 (3615), Pneumonie virale (1345), Opacit√© pulmonaire (6012). Les images proviennent de diff√©rentes sources m√©dicales internationales.
    """)

    st.subheader("Distribution des classes")
    df_dist = pd.DataFrame({
        'Classe': ['Normal', 'Opacit√© Pulmonaire', 'COVID-19', 'Pneumonie virale'],
        'Nombre d\'images': [10192, 6012, 3615, 1345]
    })

    st.table(df_dist.set_index("Classe"))

    fig = px.bar(
        df_dist,
        x='Classe',
        y="Nombre d'images",
        text="Nombre d'images",
        color='Classe',
        title="R√©partition des classes dans le jeu de donn√©es",
        color_discrete_sequence=px.colors.qualitative.Set2
    )
    fig.update_traces(textposition='outside')
    fig.update_layout(
        xaxis_title="Classe",
        yaxis_title="Nombre d'images",
        showlegend=False
    )
    st.plotly_chart(fig, use_container_width=True)

    st.subheader("Analyse exploratoire")
    st.write("""
    PCA, UMAP, autoencodeurs et histogrammes ont permis de visualiser la structure latente du jeu de donn√©es. Des anomalies ont √©t√© identifi√©es, telles que des doublons ou des images de faible qualit√©.

    La distribution est in√©gale, avec 48% de radios normales et seulement 6% de pneumonies virales, ce qui peut poser des d√©fis pour l'apprentissage automatique.
    """)
    interactive_image("../images/DistributionClasses.png", "exemple")

    st.write("""
    Visualisation statistique : variance de l‚Äôintensit√©, projections UMAP, et examen manuel sur quelques images.
    Variance : ci-dessous une visualisation de la variance par classe
    """)
    interactive_image("../images/Variance.png", "exemple")

    st.write("Intensit√© vs. √©cart-type : ci-dessous une visualisation de la r√©partition de l‚Äôintensit√© en fonction de l‚Äô√©cart-type sur les radios apr√®s normalisation :")
    interactive_image("../images/Intensite-ecart.png", "exemple")

    st.write("""
    Projection 2D apr√®s r√©duction de dimension via PCA (lin√©aire) et normalisation pr√©alable :
    """)
    interactive_image("../images/Projection2d.png", "exemple")

    st.write("""
    Projection 2D apr√®s r√©duction de dimension via UMAP non lin√©aire (High Performance Dimension Reduction) et normalisation pr√©alable:
    """)
    interactive_image("../images/UMAP.png", "exemple")

    st.write("""
    Projection 2D apr√®s encodage / d√©codage par auto-encodeur (AE) et: normalisation pr√©alable
    """)
    interactive_image("../images/Autoencoder.png", "exemple")

    st.write("""
    Inspection visuelle de quelques images : l‚Äôinspection visuelle met en √©vidence que les radios sont dans l‚Äôensemble de tr√®s bonne qualit√©.
    """)
    interactive_image("../images/InspectionVisuelle.png", "exemple")

    st.subheader("Pr√©traitement")
    st.write("""
    Les images ont √©t√© redimensionn√©es √† 240x240 pixels, normalis√©es, et enrichies par augmentation de donn√©es (flip, rotation, zoom). Des m√©thodes comme Isolation Forest ont √©t√© utilis√©es pour retirer les outliers.

    Il a √©t√© constat√© que 7 radiographies sur 10 ne sont pas normalis√©es. Voici la repr√©sentation en fonction des diverses sources de donn√©es initiales :            
    """)
    interactive_image("../images/Normalisation.png", "exemple")
    

elif section == "3. M√©thodologie":
    st.title("M√©thodologie")

    st.subheader("Classification du probl√®me")
    st.write("""
    Il s‚Äôagit d‚Äôun probl√®me de classification multi-classes supervis√©e, relevant du
    domaine de la sant√© et plus pr√©cis√©ment du diagnostic radio assist√© par IA.
    """)

    st.subheader("Pipeline g√©n√©ral")
    st.markdown("""
    1. **Chargement et pr√©traitement des images**  
    2. **√âchantillonnage √©quilibr√© ou pond√©ration des classes dans les mod√®les**  
    3. **Entra√Ænement d‚Äôalgorithmes de machine learning / deep learning**  
    4. **Optimisation des mod√®les et des hyperparam√®tres**  
    5. **√âvaluation crois√©e, analyse d‚Äôerreurs, interpr√©tation visuelle**
    """)


elif section == "4. Modelisation":
    st.title("Mod√®les de Machine Learning")
    st.header("R√©sultats des mod√®les classiques")

    model_info = {
        "KNN": {
            "description": """KNN est un algorithme d‚Äôapprentissage supervis√© introduit dans les ann√©es 1950. Il pr√©dit la classe d‚Äôun √©chantillon en se basant sur les **K voisins les plus proches** dans l‚Äôespace des caract√©ristiques, en utilisant une m√©trique de distance comme la distance euclidienne.""",
            "metrics": "**F1-score : 77 %, Accuracy : 83 %**"
        },
        "Random Forest": {
            "description": """Random Forest, propos√© par Leo Breiman en 2001, est un ensemble d‚Äô**arbres de d√©cision** entra√Æn√©s sur des sous-√©chantillons al√©atoires des donn√©es (m√©thode bootstrap). Il utilise √©galement une s√©lection al√©atoire de variables pour r√©duire la corr√©lation entre les arbres, ce qui permet de **r√©duire la variance globale** du mod√®le.""",
            "metrics": "**F1-score : 83 %, Accuracy : 86 %**"
        },
        "SVM": {
            "description": """SVM est un algorithme de classification supervis√©e qui cherche √† **maximiser la marge** entre les classes en trouvant l‚Äôhyperplan optimal. Il est particuli√®rement efficace pour les probl√®mes lin√©airement s√©parables ou faiblement bruit√©s.""",
            "metrics": "**F1-score : 82 %, Accuracy : 85 %**"
        },
        "XGBoost": {
            "description": """XGBoost est une m√©thode d‚Äôensemble bas√©e sur le **gradient boosting**, introduite par Tianqi Chen en 2016. Il construit une s√©quence de mod√®les faibles (arbres peu profonds) o√π chaque mod√®le suivant corrige les erreurs du pr√©c√©dent. Il est reconnu pour son **efficacit√© et performance en comp√©tition**.""",
            "metrics": "**F1-score : 86 %, Accuracy : 88 %**"
        },
        "MLPClassifier": {
            "description": """MLPClassifier est un **r√©seau de neurones artificiels** √† propagation avant (feedforward) compos√© de plusieurs couches : entr√©e, cach√©e(s) et sortie. Chaque neurone applique une fonction d‚Äôactivation non lin√©aire, et l‚Äôapprentissage est r√©alis√© par **r√©tropropagation du gradient** (souvent avec Adam ou SGD).""",
            "metrics": "**F1-score : 81 %, Accuracy : 84 %**"
        }
    }

    selected_model = st.selectbox("S√©lectionnez un mod√®le pour afficher les d√©tails :", list(model_info.keys()))

    st.subheader(selected_model)
    st.write(model_info[selected_model]["description"])
    st.markdown(model_info[selected_model]["metrics"])
    st.markdown("---")

    st.write("Conclusion et tableau de synth√®se")
    st.image("../images/TablaModelos.png", caption="exemple", width=750)

    # Contin√∫a con el resto de las subsecciones como estaban
    st.title("Optimisation des mod√®les ML")

    st.subheader("üîç Grid Search")
    st.write("""Grid Search est une m√©thode d‚Äôoptimisation des hyperparam√®tres...""")

    st.subheader("üìà HOG (Histogram of Oriented Gradients)")
    st.write("""Le descripteur HOG, introduit par Dalal et Triggs en 2005...""")

    st.subheader("‚öñÔ∏è Standardisation des donn√©es")
    st.write("""La standardisation met les variables sur des √©chelles comparables...""")

    st.subheader("üñºÔ∏è Effet de la taille des images")
    st.write("""Une image plus grande contient davantage d‚Äôinformation visuelle...""")

    st.subheader("üìä Panel de donn√©es et impact du sampling")
    st.write("### Undersampling")
    st.write("""Une r√©duction al√©atoire de la taille du jeu d‚Äôentra√Ænement montre que...""")

    st.write("### Oversampling avec SMOTE")
    st.write("""La technique **SMOTE** permet de g√©n√©rer artificiellement...""")



elif section == "5. Mod√®les de Deep Learning":
    st.title("Mod√®les de Deep Learning")

    st.write("""
Les mod√®les **CNN pr√©entra√Æn√©s fine-tun√©s** ont largement surpass√© les mod√®les classiques de machine learning, gr√¢ce √† leur capacit√© √† capturer des caract√©ristiques complexes dans les images m√©dicales.
""")

    st.markdown("---")
    st.write("## üß† Mod√®les explor√©s")

    with st.expander("üìå VGG16"):
        st.write("""
D√©velopp√© par l‚Äô√©quipe du Visual Geometry Group (VGG) √† l‚ÄôUniversit√© d‚ÄôOxford, VGG16 a √©t√© propos√© en 2014 et a marqu√© une avanc√©e majeure dans la vision par ordinateur. Son architecture simple et profonde repose sur des **convolutions 3x3 empil√©es**.  
**F1-score : 99.31 %, Accuracy : 99.31 %**
""")

    with st.expander("üìå InceptionV3"):
        st.write("""
Mod√®le introduit par Google en 2015, InceptionV3 am√©liore les versions pr√©c√©dentes d‚ÄôInception/GoogLeNet. Il utilise des blocs "Inception" compos√©s de **convolutions de diff√©rentes tailles**, ce qui permet de capter plusieurs √©chelles d'information simultan√©ment.  
**F1-score : 99.02 %, Accuracy : 99.02 %**
""")

    with st.expander("üìå LeNet-5"):
        st.write("""
L‚Äôun des tout premiers CNN op√©rationnels, propos√© par Yann LeCun en 1998. Utilis√© initialement pour la reconnaissance de chiffres manuscrits (MNIST), LeNet est un mod√®le simple mais historique, ayant pos√© les bases du deep learning moderne.  
**F1-score : 91 %, Accuracy : 93 %**
""")

    with st.expander("üìå ResNet"):
        st.write("""
Propos√© en 2015 par Kaiming He (Microsoft Research), ResNet introduit les **connexions r√©siduelles**, qui permettent d‚Äôentra√Æner des r√©seaux tr√®s profonds sans perte de performance. Cette innovation a r√©volutionn√© l'apprentissage profond.  
**F1-score : 99.19 %, Accuracy : 99.19 %**
""")

    with st.expander("üìå EfficientNetB0"):
        st.write("""
Pr√©sent√© par Google Brain en 2019, EfficientNet introduit un **scaling uniforme** des dimensions (profondeur, largeur, r√©solution) du r√©seau. Il atteint une **meilleure efficacit√© et pr√©cision** avec un nombre de param√®tres r√©duit.  
**F1-score : 99.08 %, Accuracy : 99.08 %**
""")

    with st.expander("üìå DenseNet-121"):
        st.write("""
Propos√© en 2017 par Gao Huang, DenseNet se distingue par sa **connectivit√© dense entre les couches**. Chaque couche re√ßoit comme entr√©e les sorties de toutes les couches pr√©c√©dentes dans le bloc. Cette strat√©gie favorise une meilleure r√©utilisation des caract√©ristiques extraites.  
**F1-score : 99.04 %, Accuracy : 99.04 %**
""")

    st.write("""
### ‚úÖ Conclusion et tableau de synth√®se
Nette am√©lioration par rapport aux mod√®les de machine learning classiques : **F1-score global > 98‚ÄØ%** pour la classification 3 classes (hors LeNet qui est √† 90‚ÄØ%).
""")
    st.image("../images/DeepSynthese.png", caption="Synth√®se des performances des mod√®les CNN", width=750)

    st.markdown("---")
    st.subheader("üîß Optimisation des mod√®les deep learning")

    with st.expander("üìê Effet de la taille des images"):
        st.write("""
Le graphique ci-dessus illustre l‚Äô√©volution de la pr√©cision et de la loss pour diff√©rentes tailles d‚Äôimages (32√ó32, 64√ó64, 128√ó128, 240√ó240), en fonction du nombre d‚Äô√©poques.  
On observe un gain notable en pr√©cision de validation, passant de **~80 %** avec des images 32√ó32 √† **plus de 90 %** avec des images 240√ó240.  
Contrairement aux mod√®les classiques, les CNN b√©n√©ficient d‚Äôimages en haute r√©solution.  
**‚û°Ô∏è Les images 240√ó240 offrent le meilleur compromis performance/pr√©cision.**
""")

    with st.expander("üö´ Impact de la classe d‚Äôopacit√© pulmonaire"):
        st.write("""
Dans la classification 4 classes, la classe d‚Äôopacit√© pulmonaire n‚Äôest correctement pr√©dite que dans **82 %** des cas, bien en dessous des autres.  
Elle regroupe des pathologies non-COVID tr√®s diverses et peu homog√®nes.  
En retirant cette classe, la classification (3 classes) gagne en pr√©cision (souvent >95‚ÄØ%).  
**‚û°Ô∏è D√©cision : retirer la classe d‚Äôopacit√© pulmonaire pour am√©liorer la clart√© du mod√®le.**
""")
        st.image("../images/umap_sans.png", caption="Repr√©sentation UMAP sans la classe d‚Äôopacit√©", width=700)

    with st.expander("üîç Optimisation des hyperparam√®tres avec Optuna / Keras Tuner"):
        st.write("""
L‚Äôoptimisation des hyperparam√®tres sur EfficientNet a permis un gain significatif de performance :
- üìà Scores par classe jusqu‚Äô√† **99 %** (contre 95‚ÄØ% sans tuning)
- üß™ Tuning effectu√© sur :  
  - le **learning rate**  
  - la **taille des couches denses**  
  - le **dropout**

‚û°Ô∏è L'impact est particuli√®rement visible dans les matrices de confusion apr√®s tuning.
""")

    with st.expander("üò∑ Effet des masques sur les performances"):
        st.write("""
Test effectu√© avec LeNet sur deux jeux de donn√©es : avec et sans masques.  
R√©sultat :  
- Les masques entra√Ænent une **d√©gradation syst√©matique** des performances (Pr√©cision, Rappel, F1).  
- Cela pourrait s‚Äôexpliquer par la **perte d‚Äôinformations cl√©s** dans la zone du visage ou du thorax.

**‚û°Ô∏è Conclusion : l‚Äôusage des masques, dans ce cas, n‚Äôest pas b√©n√©fique pour l'entra√Ænement.**
""")




elif section == "6. Mod√®les Avanc√©s":
    st.title("Mod√®les Avanc√©s")
    st.write("""
Les mod√®les avanc√©s explorent la combinaison de **CNN classiques avec des modules d‚Äôattention** ou des architectures de type **Transformer**.
""")
    st.markdown("---")
    st.subheader("üî¨ Architectures hybrides explor√©es")

    with st.expander("üß† DenseNet-121 + Vision Transformer (ViT)"):
        st.write("""
DenseNet-121 est un r√©seau CNN structur√© en quatre blocs denses. Chaque couche re√ßoit les sorties de toutes les couches pr√©c√©dentes, am√©liorant ainsi la r√©utilisation des caract√©ristiques extraites et limitant la disparition du gradient.  
ViT (Vision Transformer), quant √† lui, segmente l‚Äôimage en **patches** pour appliquer une **self-attention globale**.  
Cette approche hybride exploite **les forces combin√©es** :  
- locales ‚Üí CNN  
- globales ‚Üí Transformer  

**F1-score pond√©r√© : 0.99, Pr√©cision pond√©r√©e : 0.99**  
Optimis√© via **Grid Search** sur :  
- le learning rate  
- le taux de dropout  
- la taille des batchs
""")

    with st.expander("üì¶ VGG16 + SE (Squeeze-and-Excitation)"):
        st.write("""
VGG16 est un CNN profond √† 16 couches, introduit en 2014.  
Les blocs SE (Squeeze-and-Excitation) appliquent une **attention par canal**, permettant au r√©seau de **recalibrer dynamiquement** les canaux d‚Äôactivation.  
Cette attention est ins√©r√©e apr√®s chaque bloc convolutif, avant le max-pooling.

**F1-score pond√©r√© : 0.9864, Pr√©cision pond√©r√©e : 0.9864**
""")

    st.markdown("---")
    st.subheader("üìâ Conclusion")
    st.write("""
Contrairement aux attentes, ces mod√®les avanc√©s **ne surpassent pas significativement** les mod√®les CNN fine-tun√©s comme EfficientNet ou InceptionV3.  
En revanche, leur co√ªt en calcul est **nettement plus √©lev√©**, notamment √† cause des modules d‚Äôattention.
""")


elif section == "7. R√©sultats et Interpr√©tation":
    st.title("R√©sultats et Interpr√©tation")
    
    st.subheader("Analyse des erreurs")
    st.write("""
    La classe 'Opacit√© pulmonaire' introduit beaucoup de confusion. Les erreurs sont rares sur 'COVID' et 'Normal'.
    """)

    st.subheader("Grad-CAM et interpr√©tabilit√©")
    st.write("""
    Visualisation des zones activ√©es par le mod√®le. Bonnes correspondances avec les zones pulmonaires atteintes.
    
    L‚Äôimage ci-dessous montre une carte Grad-CAM produite par EfficientNet pour une image class√©e comme COVID. 
    On observe une activation marqu√©e dans la r√©gion inf√©rieure gauche du poumon ‚Äî typique des atteintes li√©es √† la COVID-19.
    
    Attention toutefois : l‚Äôinterpr√©tation de ces cartes d‚Äôactivation reste complexe dans un contexte m√©dical.
    """)

    st.image("images/gradcam_covid.png", caption="Activation Grad-CAM sur une image class√©e comme COVID", width=400)

elif section == "8. Bilan et perspectives":
    st.title("Bilan et perspectives")

    st.subheader("R√©sultats obtenus")
    st.write("""
Les mod√®les test√©s atteignent globalement d'excellents r√©sultats (autour de 99 % de F1-score).
Le mod√®le **EfficientNetB0** a √©t√© retenu pour la d√©mo finale en raison de son **excellent compromis entre performance et co√ªt d'entra√Ænement**.  
Il a atteint un **F1-score pond√©r√© de 99.08 %**.
    """)

    st.subheader("Difficult√©s rencontr√©es")
    st.write("""
- **Donn√©es non homog√®nes** : forte variabilit√© des images selon leur origine.
- **Classes d√©s√©quilibr√©es** : certains labels sont sous-repr√©sent√©s, rendant l'apprentissage plus difficile.
- **Infrastructure GPU n√©cessaire** : les entra√Ænements avanc√©s n√©cessitent des ressources que nos PC personnels ne permettaient pas.  
  L‚Äôusage des GPU gratuits de **Kaggle (30h/semaine)** a √©t√© essentiel pour tester le d√©gel des couches convolutives.
- **Gestion de la m√©moire GPU** : lors du d√©gel de plusieurs blocs convolutifs (ex. : VGG16 en 4 classes), il a √©t√© n√©cessaire de r√©duire la taille des batchs (batch = 32).
- **Choix de la taille des batchs** :  
  - Trop petite : mauvais apprentissage (~20 % d‚Äôaccuracy en VGG16 3 classes avec batch=32).  
  - Trop grande : overfitting ou crash m√©moire.  
  - **Compromis id√©al identifi√© autour de batch=32**, selon les r√©sultats de la recherche d‚Äôhyperparam√®tres.
- **Pr√©traitement des images** : divergence des m√©thodes selon les cas (√©limination des anomalies, contraste, bruit‚Ä¶).
- **Temps limit√©** : projet men√© en parall√®le de la formation et des obligations professionnelles.
    """)

    st.subheader("Perspectives futures")
    st.write("""
- **Int√©gration de m√©tadonn√©es cliniques** : √¢ge, sexe, ant√©c√©dents, sympt√¥mes‚Ä¶
- **Exploration de nouvelles architectures** :  
  Ex. : **Gravitational Search Algorithm** pour optimiser les hyperparam√®tres.
- **Am√©lioration de l‚Äôinterpr√©tabilit√©** :  
  Le mod√®le **DenseNet-121 + Vision Transformer** permet une meilleure compr√©hension via des **attention maps** et **Grad-CAM++**.
    """)


elif section == "9. Essai avec une radiographie":
    st.title("üß™ Essai avec une radiographie")
    uploaded_file = st.file_uploader("T√©l√©versez une radiographie", type=["jpg", "jpeg", "png"])

    @st.cache_resource
    def load_model():
        return tf.keras.models.load_model("efficientnet_final.h5")

    model = load_model()
    class_names = ["COVID", "Normal", "Viral Pneumonia"]

    def preprocess_image(image):
        image = image.convert("RGB").resize((240, 240))
        return np.expand_dims(np.array(image) / 255.0, axis=0)

    if uploaded_file is not None:
        image = Image.open(uploaded_file)
        st.image(image, caption="Image t√©l√©vers√©e", use_column_width=True)

        with st.spinner("Pr√©diction en cours..."):
            input_tensor = preprocess_image(image)
            predictions = model.predict(input_tensor)[0]
            predicted_class = class_names[np.argmax(predictions)]
            confidence = 100 * np.max(predictions)

        st.markdown(f"**Classe pr√©dite :** `{predicted_class}`")
        st.markdown(f"**Confiance :** `{confidence:.2f}%`")
        st.bar_chart(dict(zip(class_names, predictions)))
