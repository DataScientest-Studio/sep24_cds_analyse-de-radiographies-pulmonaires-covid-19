import streamlit as st
from utils import interactive_image

st.set_page_config(page_title="Mod√©lisation", layout="wide")


st.title("Mod√®les de Machine Learning")
st.header("R√©sultats des mod√®les classiques")

model_info = {
    "KNN": {
        "description": """KNN est un algorithme d‚Äôapprentissage supervis√© introduit dans les ann√©es 1950. Il pr√©dit la classe d‚Äôun √©chantillon en se basant sur les **K voisins les plus proches** dans l‚Äôespace des caract√©ristiques, en utilisant une m√©trique de distance comme la distance euclidienne.""",
        "metrics": "**F1-score : 77 %, Accuracy : 83 %**"
    },
    "Random Forest": {
        "description": """Random Forest, propos√© par Leo Breiman en 2001, est un ensemble d‚Äô**arbres de d√©cision** entra√Æn√©s sur des sous-√©chantillons al√©atoires des donn√©es (m√©thode bootstrap). Il utilise √©galement une s√©lection al√©atoire de variables pour r√©duire la corr√©lation entre les arbres, ce qui permet de **r√©duire la variance globale** du mod√®le.""",
        "metrics": "**F1-score : 83 %, Accuracy : 86 %**"
    },
    "SVM": {
        "description": """SVM est un algorithme de classification supervis√©e qui cherche √† **maximiser la marge** entre les classes en trouvant l‚Äôhyperplan optimal. Il est particuli√®rement efficace pour les probl√®mes lin√©airement s√©parables ou faiblement bruit√©s.""",
        "metrics": "**F1-score : 82 %, Accuracy : 85 %**"
    },
    "XGBoost": {
        "description": """XGBoost est une m√©thode d‚Äôensemble bas√©e sur le **gradient boosting**, introduite par Tianqi Chen en 2016. Il construit une s√©quence de mod√®les faibles (arbres peu profonds) o√π chaque mod√®le suivant corrige les erreurs du pr√©c√©dent. Il est reconnu pour son **efficacit√© et performance en comp√©tition**.""",
        "metrics": "**F1-score : 86 %, Accuracy : 88 %**"
    },
    "MLPClassifier": {
        "description": """MLPClassifier est un **r√©seau de neurones artificiels** √† propagation avant (feedforward) compos√© de plusieurs couches : entr√©e, cach√©e(s) et sortie. Chaque neurone applique une fonction d‚Äôactivation non lin√©aire, et l‚Äôapprentissage est r√©alis√© par **r√©tropropagation du gradient** (souvent avec Adam ou SGD).""",
        "metrics": "**F1-score : 81 %, Accuracy : 84 %**"
    }
}

selected_model = st.selectbox("S√©lectionnez un mod√®le pour afficher les d√©tails :", list(model_info.keys()))

st.subheader(selected_model)
st.write(model_info[selected_model]["description"])
st.markdown(model_info[selected_model]["metrics"])
st.markdown("---")

st.write("Conclusion et tableau de synth√®se")
st.image("src/images/TablaModelos.png", caption="exemple", width=750)

st.title("Optimisation des mod√®les ML")

st.subheader("üîç Grid Search")
st.write("""Grid Search est une m√©thode d‚Äôoptimisation des hyperparam√®tres...""")

st.subheader("üìà HOG (Histogram of Oriented Gradients)")
st.write("""Le descripteur HOG, introduit par Dalal et Triggs en 2005...""")

st.subheader("‚öñÔ∏è Standardisation des donn√©es")
st.write("""La standardisation met les variables sur des √©chelles comparables...""")

st.subheader("üñºÔ∏è Effet de la taille des images")
st.write("""Une image plus grande contient davantage d‚Äôinformation visuelle...""")

st.subheader("üìä Panel de donn√©es et impact du sampling")
st.write("### Undersampling")
st.write("""Une r√©duction al√©atoire de la taille du jeu d‚Äôentra√Ænement montre que...""")

st.write("### Oversampling avec SMOTE")
st.write("""La technique **SMOTE** permet de g√©n√©rer artificiellement...""")
